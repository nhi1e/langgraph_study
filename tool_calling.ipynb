{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e4ea950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "load_dotenv()\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "if \"LANGCHAIN_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangChain API key: \")\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"openai:gpt-4o\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31bd6f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"what's the weather in sf?\", additional_kwargs={}, response_metadata={}, id='cff3f852-3643-4d1a-8fcd-785f77018b69'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_2r4BeWXWOdoFhFSQrywJ8DC6', 'function': {'arguments': '{\"location\":\"San Francisco\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 51, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bs2W868yJ5JPqoaxdEgIrWcjLX6bo', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--538cc413-1cb0-4815-95ac-fa40b7e0083f-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'San Francisco'}, 'id': 'call_2r4BeWXWOdoFhFSQrywJ8DC6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 51, 'output_tokens': 15, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content=\"It's 60 degrees and foggy\", name='get_weather', id='490ef438-20ff-4479-9010-b426c2c4c3e7', tool_call_id='call_2r4BeWXWOdoFhFSQrywJ8DC6'),\n",
       "  AIMessage(content='The current weather in San Francisco is 60 degrees and foggy.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 81, 'total_tokens': 96, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bs2W9r6buo2LBnJXRp8xST3lJhvSJ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--11cb053a-3e50-457b-b0a0-6b7efd25c700-0', usage_metadata={'input_tokens': 81, 'output_tokens': 15, 'total_tokens': 96, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "\n",
    "def get_weather(location: str):\n",
    "    \"\"\"Call to get the current weather\"\"\"\n",
    "    if location.lower() in [\"sf\", \"san francisco\"]:\n",
    "        return \"It's 60 degrees and foggy\"\n",
    "    else: \n",
    "        return \"It's 90 degrees and sunny\"\n",
    "\n",
    "tool_node = ToolNode([get_weather]) # Define a tool node that can be called by the LLM\n",
    "\n",
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "def should_continue(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls: # Checks if the last message from the LLM included any tool calls.\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_conditional_edges(\"call_model\", should_continue, [\"tools\", END])\n",
    "builder.add_edge(\"tools\", \"call_model\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf?\"}]})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
