{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd5ecf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "load_dotenv()\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "if \"LANGCHAIN_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangChain API key: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64d9fec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I can't determine your name based on the information provided.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages.utils import (\n",
    "    trim_messages,\n",
    "    count_tokens_approximately\n",
    ")\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"openai:gpt-4o\",\n",
    "    temperature=0.7\n",
    ")\n",
    "summarization_model = model.bind(max_tokens=128)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=128,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "final_response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c745a09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Understanding MessagesState ===\n",
      "\n",
      "1. MessagesState definition:\n",
      "   MessagesState is a TypedDict with: {'messages': ForwardRef('Annotated[list[AnyMessage], add_messages]', module='langgraph.graph.message')}\n",
      "\n",
      "2. Checkpointer state management:\n",
      "   First call:\n",
      "   Messages after call 1: 2 messages\n",
      "   Second call (same thread_id):\n",
      "   Messages after call 2: 4 messages\n",
      "   → State automatically accumulates because of the checkpointer!\n",
      "\n",
      "3. Accumulated message history:\n",
      "   Message 1: Hello...\n",
      "   Message 2: Response to: Hello...\n",
      "   Message 3: How are you?...\n",
      "   Message 4: Response to: How are you?...\n",
      "\n",
      "Key insight: The 'messages' are stored in the checkpointer, not declared in your code!\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the MessagesState and see how state is managed\n",
    "print(\"=== Understanding MessagesState ===\")\n",
    "print()\n",
    "\n",
    "# 1. What is MessagesState?\n",
    "from langgraph.graph import MessagesState\n",
    "print(\"1. MessagesState definition:\")\n",
    "print(f\"   MessagesState is a TypedDict with: {MessagesState.__annotations__}\")\n",
    "print()\n",
    "\n",
    "# 2. How does the checkpointer work?\n",
    "print(\"2. Checkpointer state management:\")\n",
    "config = {\"configurable\": {\"thread_id\": \"demo\"}}\n",
    "\n",
    "# Let's create a new graph to demonstrate\n",
    "demo_builder = StateGraph(MessagesState)\n",
    "demo_builder.add_node(\"demo_node\", lambda state: {\"messages\": [f\"Response to: {state['messages'][-1].content}\"]})\n",
    "demo_builder.add_edge(START, \"demo_node\")\n",
    "demo_graph = demo_builder.compile(checkpointer=InMemorySaver())\n",
    "\n",
    "# First interaction\n",
    "print(\"   First call:\")\n",
    "result1 = demo_graph.invoke({\"messages\": \"Hello\"}, config)\n",
    "print(f\"   Messages after call 1: {len(result1['messages'])} messages\")\n",
    "\n",
    "# Second interaction - state persists!\n",
    "print(\"   Second call (same thread_id):\")\n",
    "result2 = demo_graph.invoke({\"messages\": \"How are you?\"}, config)\n",
    "print(f\"   Messages after call 2: {len(result2['messages'])} messages\")\n",
    "print(\"   → State automatically accumulates because of the checkpointer!\")\n",
    "print()\n",
    "\n",
    "# 3. Show the actual message history\n",
    "print(\"3. Accumulated message history:\")\n",
    "for i, msg in enumerate(result2['messages']):\n",
    "    print(f\"   Message {i+1}: {msg.content[:50]}...\")\n",
    "print()\n",
    "\n",
    "print(\"Key insight: The 'messages' are stored in the checkpointer, not declared in your code!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d3054c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Understanding trim_messages() ===\n",
      "\n",
      "1. Original message count: 7\n",
      "\n",
      "2. Trim with max_tokens=128, strategy='last':\n",
      "   Trimmed to: 7 messages\n",
      "   1. HumanMessage: Hi, my name is Bob...\n",
      "   2. AIMessage: Hello Bob! Nice to meet you....\n",
      "   3. HumanMessage: Write a short poem about cats...\n",
      "   4. AIMessage: Cats are graceful, cats are sweet......\n",
      "   5. HumanMessage: Now do the same but for dogs...\n",
      "   6. AIMessage: Dogs are loyal, dogs are true......\n",
      "   7. HumanMessage: What's my name?...\n",
      "\n",
      "3. What each parameter does:\n",
      "   - strategy='last': Keep the most recent messages within token limit\n",
      "   - max_tokens=128: Maximum tokens to keep\n",
      "   - start_on='human': Always start the trimmed conversation with a human message\n",
      "   - end_on=('human', 'tool'): End on human or tool message (ensures complete exchange)\n",
      "   - token_counter: Function to count tokens approximately\n",
      "\n",
      "4. Why trimming matters in your example:\n",
      "   - Without trimming: All 4 exchanges would be sent to the model\n",
      "   - With trimming: Only recent exchanges that fit in 128 tokens\n",
      "   - Result: The model forgets 'Bob' because early messages get trimmed!\n"
     ]
    }
   ],
   "source": [
    "# Now let's understand how trim_messages works\n",
    "print(\"=== Understanding trim_messages() ===\")\n",
    "print()\n",
    "\n",
    "# Let's examine the trim_messages function in detail\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Create a sample conversation\n",
    "sample_messages = [\n",
    "    HumanMessage(\"Hi, my name is Bob\"),\n",
    "    AIMessage(\"Hello Bob! Nice to meet you.\"),\n",
    "    HumanMessage(\"Write a short poem about cats\"),\n",
    "    AIMessage(\"Cats are graceful, cats are sweet...\"),\n",
    "    HumanMessage(\"Now do the same but for dogs\"),\n",
    "    AIMessage(\"Dogs are loyal, dogs are true...\"),\n",
    "    HumanMessage(\"What's my name?\"),\n",
    "]\n",
    "\n",
    "print(\"1. Original message count:\", len(sample_messages))\n",
    "print()\n",
    "\n",
    "# Test trim_messages with different parameters\n",
    "print(\"2. Trim with max_tokens=128, strategy='last':\")\n",
    "trimmed = trim_messages(\n",
    "    sample_messages,\n",
    "    strategy=\"last\",  # Keep the most recent messages\n",
    "    token_counter=count_tokens_approximately,\n",
    "    max_tokens=128,\n",
    "    start_on=\"human\",  # Always start with a human message\n",
    "    end_on=(\"human\", \"tool\"),  # End on human or tool message\n",
    ")\n",
    "print(f\"   Trimmed to: {len(trimmed)} messages\")\n",
    "for i, msg in enumerate(trimmed):\n",
    "    print(f\"   {i+1}. {type(msg).__name__}: {msg.content[:50]}...\")\n",
    "print()\n",
    "\n",
    "print(\"3. What each parameter does:\")\n",
    "print(\"   - strategy='last': Keep the most recent messages within token limit\")\n",
    "print(\"   - max_tokens=128: Maximum tokens to keep\")\n",
    "print(\"   - start_on='human': Always start the trimmed conversation with a human message\")\n",
    "print(\"   - end_on=('human', 'tool'): End on human or tool message (ensures complete exchange)\")\n",
    "print(\"   - token_counter: Function to count tokens approximately\")\n",
    "print()\n",
    "\n",
    "print(\"4. Why trimming matters in your example:\")\n",
    "print(\"   - Without trimming: All 4 exchanges would be sent to the model\")\n",
    "print(\"   - With trimming: Only recent exchanges that fit in 128 tokens\")\n",
    "print(\"   - Result: The model forgets 'Bob' because early messages get trimmed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c88c2",
   "metadata": {},
   "source": [
    "# Summary: Answering Your Questions\n",
    "\n",
    "## Where is the state getting declared as messages for this thread?\n",
    "\n",
    "**Answer**: The state is NOT explicitly declared in your code. Here's the flow:\n",
    "\n",
    "1. **`MessagesState`** - Pre-built TypedDict from LangGraph with `messages: Annotated[list[BaseMessage], add_messages]`\n",
    "2. **`InMemorySaver()`** - Checkpointer that automatically saves/loads state by `thread_id`\n",
    "3. **`config = {\"configurable\": {\"thread_id\": \"1\"}}`** - This tells LangGraph which conversation thread to use\n",
    "4. **Each `invoke()`** - Automatically loads existing messages, adds new ones, saves back to checkpointer\n",
    "\n",
    "## How the full flow works:\n",
    "\n",
    "```\n",
    "Call 1: graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "  → Checkpointer loads: [] (empty)\n",
    "  → Adds user message: [HumanMessage(\"hi, my name is bob\")]\n",
    "  → trim_messages() processes: [HumanMessage(\"hi, my name is bob\")]\n",
    "  → Model responds: [HumanMessage(\"hi...\"), AIMessage(\"Hello Bob...\")]\n",
    "  → Checkpointer saves state for thread \"1\"\n",
    "\n",
    "Call 2: graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "  → Checkpointer loads: [HumanMessage(\"hi...\"), AIMessage(\"Hello Bob...\")]\n",
    "  → Adds user message: [HumanMessage(\"hi...\"), AIMessage(\"Hello Bob...\"), HumanMessage(\"write a short poem...\")]\n",
    "  → trim_messages() processes and trims if needed\n",
    "  → Model responds, checkpointer saves updated state\n",
    "\n",
    "...and so on for each call\n",
    "```\n",
    "\n",
    "## Why trimming causes the name to be forgotten:\n",
    "\n",
    "With `max_tokens=128`, the early messages (including \"my name is Bob\") get trimmed out, so the model loses that context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f20e6894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Debugging the Memory Loss ===\n",
      "\n",
      "Simulating the same conversation with debug output...\n",
      "\n",
      "Full conversation has 1 messages:\n",
      "  1. HumanMessage: hi, my name is bob...\n",
      "\n",
      "After trimming to 128 tokens, only 1 messages remain:\n",
      "  1. HumanMessage: hi, my name is bob...\n",
      "\n",
      "Token count of trimmed messages: ~9\n",
      "\n",
      "Full conversation has 3 messages:\n",
      "  1. HumanMessage: hi, my name is bob...\n",
      "  2. AIMessage: Hello Bob! How can I assist you today?...\n",
      "  3. HumanMessage: write a short poem about cats...\n",
      "\n",
      "After trimming to 128 tokens, only 3 messages remain:\n",
      "  1. HumanMessage: hi, my name is bob...\n",
      "  2. AIMessage: Hello Bob! How can I assist you today?...\n",
      "  3. HumanMessage: write a short poem about cats...\n",
      "\n",
      "Token count of trimmed messages: ~36\n",
      "\n",
      "Full conversation has 3 messages:\n",
      "  1. HumanMessage: hi, my name is bob...\n",
      "  2. AIMessage: Hello Bob! How can I assist you today?...\n",
      "  3. HumanMessage: write a short poem about cats...\n",
      "\n",
      "After trimming to 128 tokens, only 3 messages remain:\n",
      "  1. HumanMessage: hi, my name is bob...\n",
      "  2. AIMessage: Hello Bob! How can I assist you today?...\n",
      "  3. HumanMessage: write a short poem about cats...\n",
      "\n",
      "Token count of trimmed messages: ~36\n",
      "\n",
      "Full conversation has 5 messages:\n",
      "  1. HumanMessage: hi, my name is bob...\n",
      "  2. AIMessage: Hello Bob! How can I assist you today?...\n",
      "  3. HumanMessage: write a short poem about cats...\n",
      "  4. AIMessage: In a sunbeam, sleek and sly,  \n",
      "A whiskered shadow passes by....\n",
      "  5. HumanMessage: now do the same but for dogs...\n",
      "\n",
      "After trimming to 128 tokens, only 1 messages remain:\n",
      "  1. HumanMessage: now do the same but for dogs...\n",
      "\n",
      "Token count of trimmed messages: ~11\n",
      "\n",
      "Full conversation has 5 messages:\n",
      "  1. HumanMessage: hi, my name is bob...\n",
      "  2. AIMessage: Hello Bob! How can I assist you today?...\n",
      "  3. HumanMessage: write a short poem about cats...\n",
      "  4. AIMessage: In a sunbeam, sleek and sly,  \n",
      "A whiskered shadow passes by....\n",
      "  5. HumanMessage: now do the same but for dogs...\n",
      "\n",
      "After trimming to 128 tokens, only 1 messages remain:\n",
      "  1. HumanMessage: now do the same but for dogs...\n",
      "\n",
      "Token count of trimmed messages: ~11\n",
      "\n",
      "Final question - this will show what the model actually sees:\n",
      "Full conversation has 7 messages:\n",
      "  1. HumanMessage: hi, my name is bob...\n",
      "  2. AIMessage: Hello Bob! How can I assist you today?...\n",
      "  3. HumanMessage: write a short poem about cats...\n",
      "  4. AIMessage: In a sunbeam, sleek and sly,  \n",
      "A whiskered shadow passes by....\n",
      "  5. HumanMessage: now do the same but for dogs...\n",
      "  6. AIMessage: Sure, I can provide information on various aspects related t...\n",
      "  7. HumanMessage: what's my name?...\n",
      "\n",
      "After trimming to 128 tokens, only 1 messages remain:\n",
      "  1. HumanMessage: what's my name?...\n",
      "\n",
      "Token count of trimmed messages: ~8\n",
      "\n",
      "Final question - this will show what the model actually sees:\n",
      "Full conversation has 7 messages:\n",
      "  1. HumanMessage: hi, my name is bob...\n",
      "  2. AIMessage: Hello Bob! How can I assist you today?...\n",
      "  3. HumanMessage: write a short poem about cats...\n",
      "  4. AIMessage: In a sunbeam, sleek and sly,  \n",
      "A whiskered shadow passes by....\n",
      "  5. HumanMessage: now do the same but for dogs...\n",
      "  6. AIMessage: Sure, I can provide information on various aspects related t...\n",
      "  7. HumanMessage: what's my name?...\n",
      "\n",
      "After trimming to 128 tokens, only 1 messages remain:\n",
      "  1. HumanMessage: what's my name?...\n",
      "\n",
      "Token count of trimmed messages: ~8\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to personal data about individuals unless it has been shared with me in this conversation. Therefore, I don't know your name. If you'd like to share it, feel free to do so!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to personal data about individuals unless it has been shared with me in this conversation. Therefore, I don't know your name. If you'd like to share it, feel free to do so!\n"
     ]
    }
   ],
   "source": [
    "# Let's see exactly what happened - why the name was forgotten\n",
    "print(\"=== Debugging the Memory Loss ===\")\n",
    "print()\n",
    "\n",
    "# Let's trace what messages were actually sent to the model in the final call\n",
    "config_debug = {\"configurable\": {\"thread_id\": \"debug\"}}\n",
    "\n",
    "# Create a debug version that shows us what trim_messages is doing\n",
    "def debug_call_model(state: MessagesState):\n",
    "    print(f\"Full conversation has {len(state['messages'])} messages:\")\n",
    "    for i, msg in enumerate(state['messages']):\n",
    "        print(f\"  {i+1}. {type(msg).__name__}: {msg.content[:60]}...\")\n",
    "    print()\n",
    "    \n",
    "    # Apply the same trimming\n",
    "    trimmed = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=128,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    \n",
    "    print(f\"After trimming to 128 tokens, only {len(trimmed)} messages remain:\")\n",
    "    for i, msg in enumerate(trimmed):\n",
    "        print(f\"  {i+1}. {type(msg).__name__}: {msg.content[:60]}...\")\n",
    "    print()\n",
    "    \n",
    "    # Count tokens for transparency\n",
    "    token_count = count_tokens_approximately(trimmed)\n",
    "    print(f\"Token count of trimmed messages: ~{token_count}\")\n",
    "    print()\n",
    "    \n",
    "    response = model.invoke(trimmed)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Build debug graph\n",
    "debug_builder = StateGraph(MessagesState)\n",
    "debug_builder.add_node(\"debug_call_model\", debug_call_model)\n",
    "debug_builder.add_edge(START, \"debug_call_model\")\n",
    "debug_graph = debug_builder.compile(checkpointer=InMemorySaver())\n",
    "\n",
    "# Recreate the same conversation\n",
    "print(\"Simulating the same conversation with debug output...\")\n",
    "print()\n",
    "debug_graph.invoke({\"messages\": \"hi, my name is bob\"}, config_debug)\n",
    "debug_graph.invoke({\"messages\": \"write a short poem about cats\"}, config_debug)\n",
    "debug_graph.invoke({\"messages\": \"now do the same but for dogs\"}, config_debug)\n",
    "print(\"Final question - this will show what the model actually sees:\")\n",
    "final_debug = debug_graph.invoke({\"messages\": \"what's my name?\"}, config_debug)\n",
    "\n",
    "final_debug[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "259f5a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FIXED VERSION - Larger Token Limit ===\n",
      "\n",
      "Testing fixed version with max_tokens=1000...\n",
      "Fixed result:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob.\n",
      "\n",
      "Key takeaway: The 128 token limit was too aggressive and trimmed away\n",
      "the important context about the user's name!\n",
      "Fixed result:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob.\n",
      "\n",
      "Key takeaway: The 128 token limit was too aggressive and trimmed away\n",
      "the important context about the user's name!\n"
     ]
    }
   ],
   "source": [
    "# Let's fix the problem - Demo with larger max_tokens\n",
    "print(\"=== FIXED VERSION - Larger Token Limit ===\")\n",
    "print()\n",
    "\n",
    "def fixed_call_model(state: MessagesState):\n",
    "    # Increase max_tokens to preserve more context\n",
    "    messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=300,  # Much larger token limit\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Create fixed graph\n",
    "fixed_builder = StateGraph(MessagesState)\n",
    "fixed_builder.add_node(\"fixed_call_model\", fixed_call_model)\n",
    "fixed_builder.add_edge(START, \"fixed_call_model\")\n",
    "fixed_graph = fixed_builder.compile(checkpointer=InMemorySaver())\n",
    "\n",
    "# Test the fixed version\n",
    "config_fixed = {\"configurable\": {\"thread_id\": \"fixed\"}}\n",
    "print(\"Testing fixed version with max_tokens=1000...\")\n",
    "\n",
    "fixed_graph.invoke({\"messages\": \"hi, my name is bob\"}, config_fixed)\n",
    "fixed_graph.invoke({\"messages\": \"write a short poem about cats\"}, config_fixed)\n",
    "fixed_graph.invoke({\"messages\": \"now do the same but for dogs\"}, config_fixed)\n",
    "fixed_result = fixed_graph.invoke({\"messages\": \"what's my name?\"}, config_fixed)\n",
    "\n",
    "print(\"Fixed result:\")\n",
    "fixed_result[\"messages\"][-1].pretty_print()\n",
    "\n",
    "print()\n",
    "print(\"Key takeaway: The 128 token limit was too aggressive and trimmed away\")\n",
    "print(\"the important context about the user's name!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
